import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time

# è®¾ç½®æ—¶åŒº UTC+9
JST_OFFSET = datetime.timedelta(hours=9)

# æ”¹ç”¨ Yahoo è¯„è®ºæ’è¡Œæ¦œ RSS (çƒ­åº¦æœ€é«˜)
RSS_URL = "https://news.yahoo.co.jp/rss/ranking/comment/all.xml"

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

def update_news():
    print("ğŸš€ å¼€å§‹æŠ“å– Yahoo è¯„è®ºæ’è¡Œæ¦œ...")
    try:
        feed = feedparser.parse(RSS_URL)
    except Exception as e:
        print(f"âŒ RSSæŠ“å–å¤±è´¥: {e}")
        return

    translator = GoogleTranslator(source='auto', target='zh-CN')
    
    # 1. è¯»å–ä»Šæ—¥å·²æœ‰çš„å­˜æ¡£ï¼ˆä¸ºäº†å»é‡ï¼‰
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
        
    date_str = get_current_jst_time().strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{date_str}.json")
    
    existing_links = set()
    current_archive_data = []

    # å¦‚æœä»Šå¤©å·²ç»æœ‰å­˜æ¡£ï¼Œå…ˆè¯»å‡ºæ¥
    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                current_archive_data = json.load(f)
                for item in current_archive_data:
                    existing_links.add(item['link'])
        except:
            print("âš ï¸ è¯»å–æ—§å­˜æ¡£å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°å­˜æ¡£")

    # 2. å¤„ç†æ–°æŠ“å–çš„æ•°æ®
    new_items_count = 0
    
    # æˆ‘ä»¬åªçœ‹ RSS çš„å‰ 15 æ¡ï¼ˆçƒ­åº¦æœ€é«˜çš„ï¼‰
    for entry in feed.entries[:15]:
        link = entry.link
        
        # å»é‡ï¼šå¦‚æœè¿™ä¸ªé“¾æ¥ä»Šå¤©å·²ç»å­˜è¿‡äº†ï¼Œå°±è·³è¿‡
        if link in existing_links:
            continue

        try:
            zh_title = translator.translate(entry.title)
        except:
            zh_title = entry.title
        
        # å°è¯•æå–å›¾ç‰‡ (Yahoo RSS æ ¼å¼ä¸å®šï¼Œå°è¯•å‡ ç§å¸¸è§å­—æ®µ)
        image_url = ""
        # 1. å°è¯• media_thumbnail
        if 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
            image_url = entry.media_thumbnail[0]['url']
        # 2. å°è¯• links ä¸­çš„ image ç±»å‹
        elif 'links' in entry:
            for l in entry.links:
                if 'image' in l.get('type', ''):
                    image_url = l['href']
                    break
        
        # è·å–æ—¶é—´
        time_str = get_current_jst_time().strftime("%H:%M")

        item_data = {
            "title": zh_title,
            "origin": entry.title,
            "link": link,
            "time": time_str,
            "image": image_url  # æ–°å¢å›¾ç‰‡å­—æ®µ
        }
        
        # æ·»åŠ åˆ°åˆ—è¡¨å¤´éƒ¨ï¼ˆæœ€æ–°çš„æ’å‰é¢ï¼‰
        current_archive_data.insert(0, item_data)
        existing_links.add(link)
        new_items_count += 1
        
        # ç¨å¾®å»¶æ—¶
        time.sleep(0.5)

    print(f"âœ… æ–°å¢äº† {new_items_count} æ¡æ–°é—»")

    # 3. ä¿å­˜ä»Šæ—¥å­˜æ¡£ (åŒ…å«ä¹‹å‰å’Œæ–°å¢çš„)
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(current_archive_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å†å²å­˜æ¡£å·²æ›´æ–°: {archive_path}")

    # 4. æ›´æ–°é¦–é¡µ data.json (åªæ˜¾ç¤ºå­˜æ¡£é‡Œæœ€æ–°çš„ 20 æ¡ï¼Œä¿æŒé¦–é¡µç²¾ç®€)
    # é¦–é¡µæ•°æ®ç›´æ¥ç”¨ä»Šå¤©çš„å­˜æ¡£å³å¯
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(current_archive_data[:20], f, ensure_ascii=False, indent=2)
    print("âœ… data.json æ›´æ–°æˆåŠŸ")

if __name__ == "__main__":
    update_news()