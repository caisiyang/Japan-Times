import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time
import requests  # å¼•å…¥ requests åº“æ¥åšä¼ªè£…

# è®¾ç½®æ—¶åŒº UTC+9
JST_OFFSET = datetime.timedelta(hours=9)

RSS_URL = "https://news.yahoo.co.jp/rss/ranking/comment/all.xml"

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

def update_news():
    print("ğŸš€ å¼€å§‹æŠ“å– Yahoo è¯„è®ºæ’è¡Œæ¦œ...")
    
    # --- ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä¼ªè£…æˆæµè§ˆå™¨ ---
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        # å…ˆç”¨ requests å¸¦ç€ä¼ªè£…å¤´å»è¯·æ±‚
        response = requests.get(RSS_URL, headers=headers, timeout=10)
        # æ‰“å°çŠ¶æ€ç ï¼Œæ–¹ä¾¿è°ƒè¯• (200è¡¨ç¤ºæˆåŠŸï¼Œ403è¡¨ç¤ºè¢«æ‹’)
        print(f"ğŸ“¡ Yahoo å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code != 200:
            print("âŒ è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½IPè¢«å°é”")
            return

        # æŠŠè¯·æ±‚åˆ°çš„å†…å®¹å–‚ç»™ feedparser
        feed = feedparser.parse(response.content)
        
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return
    # ----------------------------------

    if not feed.entries:
        print("âš ï¸ è·å–åˆ°çš„ RSS å†…å®¹ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æº")
        return

    translator = GoogleTranslator(source='auto', target='zh-CN')
    
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
        
    date_str = get_current_jst_time().strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{date_str}.json")
    
    existing_links = set()
    current_archive_data = []

    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                current_archive_data = json.load(f)
                for item in current_archive_data:
                    existing_links.add(item['link'])
        except:
            pass

    new_items_count = 0
    
    for entry in feed.entries[:15]:
        link = entry.link
        if link in existing_links:
            continue

        try:
            zh_title = translator.translate(entry.title)
        except:
            zh_title = entry.title
        
        image_url = ""
        if 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
            image_url = entry.media_thumbnail[0]['url']
        elif 'links' in entry:
            for l in entry.links:
                if 'image' in l.get('type', ''):
                    image_url = l['href']
                    break
        
        time_str = get_current_jst_time().strftime("%H:%M")

        item_data = {
            "title": zh_title,
            "origin": entry.title,
            "link": link,
            "time": time_str,
            "image": image_url
        }
        
        current_archive_data.insert(0, item_data)
        existing_links.add(link)
        new_items_count += 1
        time.sleep(0.5)

    print(f"âœ… æ–°å¢äº† {new_items_count} æ¡æ–°é—»")

    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(current_archive_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… å†å²å­˜æ¡£å·²æ›´æ–°: {archive_path}")

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(current_archive_data[:20], f, ensure_ascii=False, indent=2)
    print("âœ… data.json æ›´æ–°æˆåŠŸ")

if __name__ == "__main__":
    update_news()