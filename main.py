import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time
import requests
from bs4 import BeautifulSoup

# è®¾ç½®æ—¶åŒº UTC+9
JST_OFFSET = datetime.timedelta(hours=9)

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

def extract_image(entry):
    # å°è¯•æå–å›¾ç‰‡çš„é€»è¾‘ (Google News ä¸“ç”¨)
    content_html = ""
    if 'summary' in entry:
        content_html = entry.summary
    elif 'description' in entry:
        content_html = entry.description
    
    if content_html:
        try:
            soup = BeautifulSoup(content_html, 'html.parser')
            img = soup.find('img')
            if img and 'src' in img.attrs:
                return img['src']
        except:
            pass
    return ""

def classify_news(title):
    # ç®€å•çš„å…³é”®è¯åˆ†ç±»
    keywords = {
        "æ—¶æ”¿": ["æ”¿åºœ", "æ”¿ç­–", "ä¹ è¿‘å¹³", "æå¼º", "å¤–äº¤", "æ”¿æ²»", "é€‰ä¸¾", "è®®å‘˜", "é¦–ç›¸", "æ€»ç»Ÿ", "å†›äº‹", "å›½é˜²", "ä¸­å…±", "å…š", "å°æ¹¾", "é¦™æ¸¯", "äººæƒ", "åˆ¶è£", "å¤§ä½¿", "é¢†äº‹", "æ¡çº¦", "åå®š", "å³°ä¼š", "ä¼šè°ˆ", "å¤§è‡£", "å†…é˜", "å›½ä¼š", "å‚è®®é™¢", "ä¼—è®®é™¢", "è‡ªæ°‘å…š", "å…¬æ˜å…š", "ç«‹å®ªæ°‘ä¸»å…š", "ç»´æ–°ä¼š", "å…±äº§å…š", "å›½æ°‘æ°‘ä¸»å…š", "ä»¤å’Œæ–°é€‰ç»„", "ç¤¾æ°‘å…š", "å‚æ”¿å…š", "æ‹œç™»", "ç‰¹æœ—æ™®", "æ™®äº¬", "å²¸ç”°", "çŸ³ç ´", "é«˜å¸‚", "å°æ³‰", "æ²³é‡", "æ—èŠ³æ­£", "èŒ‚æœ¨", "åŠ è—¤", "ä¸Šå·", "æ ¸æ­¦å™¨", "å¯¼å¼¹", "æ¼”ä¹ ", "å·¡é€»", "æµ·è­¦", "é’“é±¼å²›", "å°–é˜", "å—æµ·", "ä¸œæµ·", "å°æµ·"],
        "ç»æµ": ["ç»æµ", "è´¸æ˜“", "è‚¡å¸‚", "æŠ•èµ„", "é“¶è¡Œ", "ä¼ä¸š", "GDP", "å¸‚åœº", "æ¶ˆè´¹", "äº§ä¸š", "æ±‡ç‡", "ç¾å…ƒ", "æ—¥å…ƒ", "ç”µåŠ¨è½¦", "EV", "åŠå¯¼ä½“", "èŠ¯ç‰‡", "é€šèƒ€", "ç‰©ä»·", "å·¥èµ„", "å°±ä¸š", "å¤±ä¸š", "æˆ¿åœ°äº§", "æ¥¼å¸‚", "å¤®è¡Œ", "åˆ©ç‡", "åŠ æ¯", "é™æ¯", "å…³ç¨", "å‡ºå£", "è¿›å£", "ä¾›åº”é“¾", "åˆ¶é€ ", "ä¸°ç”°", "æœ¬ç”°", "æ—¥äº§", "ç´¢å°¼", "æ¾ä¸‹", "è½¯é“¶", "ä¼˜è¡£åº“", "ä»»å¤©å ‚", "é˜¿é‡Œ", "è…¾è®¯", "å­—èŠ‚", "åä¸º", "æ¯”äºšè¿ª", "å®å¾·æ—¶ä»£", "è´¢æŠ¥", "äºæŸ", "ç›ˆåˆ©", "æ”¶è´­", "åˆå¹¶", "ç ´äº§", "è£å‘˜"],
        "ç¤¾ä¼š": ["ç¤¾ä¼š", "äººå£", "æ•™è‚²", "åŒ»ç–—", "çŠ¯ç½ª", "äº‹æ•…", "ç¾å®³", "ç–«æƒ…", "ç”Ÿæ´»", "æ—…æ¸¸", "ç­¾è¯", "ç§»æ°‘", "å°‘å­åŒ–", "è€é¾„åŒ–", "å…»è€", "ç¦åˆ©", "ä¿é™©", "åŒ»é™¢", "åŒ»ç”Ÿ", "æŠ¤å£«", "å­¦æ ¡", "å­¦ç”Ÿ", "è€å¸ˆ", "å¤§å­¦", "é«˜è€ƒ", "ç•™å­¦", "æ²»å®‰", "è­¦å¯Ÿ", "é€®æ•", "å®¡åˆ¤", "æ³•é™¢", "å¾‹å¸ˆ", "åœ°éœ‡", "å°é£", "æš´é›¨", "æ´ªæ°´", "ç«ç¾", "äº¤é€š", "é“è·¯", "æ–°å¹²çº¿", "èˆªç­", "æœºåœº", "åœ°é“", "å…¬äº¤", "å‡ºç§Ÿè½¦", "é£Ÿå“", "å®‰å…¨", "ç¯å¢ƒ", "æ±¡æŸ“", "åƒåœ¾", "æ°”å€™", "å˜æš–", "ç¢³ä¸­å’Œ", "æ ¸ç”µ", "æ ¸æ±¡æ°´", "æ’æµ·", "é–å›½ç¥ç¤¾", "ç†ŠçŒ«"],
        "ä½“è‚²": ["ä½“è‚²", "å¥¥è¿", "è¶³çƒ", "ç¯®çƒ", "æ£’çƒ", "é€‰æ‰‹", "æ¯”èµ›", "å† å†›", "å¤§è°·", "ç¿”å¹³", "ç¾½ç”Ÿ", "ç»“å¼¦", "ä¹’ä¹“", "ç½‘çƒ", "æ¸¸æ³³", "ç”°å¾„", "é©¬æ‹‰æ¾", "ç›¸æ‰‘", "æŸ”é“", "ç©ºæ‰‹é“", "å‰‘é“", "ä¸–ç•Œæ¯", "äºšæ´²æ¯", "äºšè¿ä¼š", "è”èµ›", "ä¿±ä¹éƒ¨", "çƒé˜Ÿ", "é‡‘ç‰Œ", "é“¶ç‰Œ", "é“œç‰Œ"],
        "ç§‘æŠ€": ["ç§‘æŠ€", "ç§‘å­¦", "AI", "äº’è”ç½‘", "æ‰‹æœº", "èŠ¯ç‰‡", "èˆªå¤©", "ç ”å‘", "åŠå¯¼ä½“", "äººå·¥æ™ºèƒ½", "æœºå™¨äºº", "æ— äººæœº", "5G", "6G", "å«æ˜Ÿ", "ç«ç®­", "æ¢æµ‹", "å®‡å®™", "å¤ªç©º", "ç”Ÿç‰©", "åŸºå› ", "ç–«è‹—", "è¯ç‰©", "ç™Œç—‡", "è¯ºè´å°”", "ç‰©ç†", "åŒ–å­¦", "æ•°å­¦", "å¤©æ–‡", "é»‘æ´", "é‡å­", "è¶…å¯¼", "ææ–™", "ç”µæ± ", "èƒ½æº", "æ¸…æ´", "ç¯ä¿", "è‡ªåŠ¨é©¾é©¶", "å…ƒå®‡å®™", "åŒºå—é“¾", "åŠ å¯†è´§å¸", "æ¯”ç‰¹å¸"],
        "å¨±ä¹": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ˜æ˜Ÿ", "åŠ¨æ¼«", "æ¸¸æˆ", "åŠ¨ç”»", "æ¼«ç”»", "å£°ä¼˜", "å¶åƒ", "æ­Œæ‰‹", "æ¼”å‘˜", "å¯¼æ¼”", "å‰§é›†", "æ—¥å‰§", "éŸ©å‰§", "ç»¼è‰º", "æ¼”å”±ä¼š", "ç¥¨æˆ¿", "æ¦œå•", "çº¢ç™½", "å‰åœåŠ›", "å®«å´éª", "æ–°æµ·è¯š", "é¬¼ç­", "å’’æœ¯", "æµ·è´¼ç‹", "ç«å½±", "æŸ¯å—", "å®å¯æ¢¦", "é©¬é‡Œå¥¥", "å¡å°”è¾¾"]
    }
    
    for category, words in keywords.items():
        for word in words:
            if word in title:
                return category
    return "å…¶ä»–"

def fetch_google_china_news():
    print("ğŸš€ æ­£åœ¨æŠ“å– Google News (æ—¥æœ¬/ä¸­å›½ç›¸å…³)...")
    # å…³é”®è¯ï¼šä¸­å›½
    # ceid=JP:ja é™åˆ¶ä¸ºæ—¥æœ¬ç‰ˆ
    # when:1d é™åˆ¶è¿‡å»24å°æ—¶ (æˆ‘ä»¬æ¯å¤©å­˜ï¼Œé¦–é¡µèšåˆ7å¤©ï¼Œæ‰€ä»¥æŠ“24å°æ—¶å¤Ÿäº†)
    url = "https://news.google.com/rss/search?q=ä¸­å›½+when:1d&hl=ja&gl=JP&ceid=JP:ja"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return []
        feed = feedparser.parse(response.content)
        return feed.entries
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return []

def process_entries(entries):
    processed = []
    translator = GoogleTranslator(source='auto', target='zh-CN')
    
    # æ¯æ¬¡æœ€å¤šæŠ“ 30 æ¡
    for entry in entries[:30]:
        original_title = entry.title
        # å»æ‰åª’ä½“åç¼€ (ä¾‹å¦‚ " - NHK NEWS")
        clean_title = original_title.split(' - ')[0]
        
        try:
            zh_title = translator.translate(clean_title)
        except:
            zh_title = clean_title 

        image_url = extract_image(entry)
        category = classify_news(zh_title) # è‡ªåŠ¨åˆ†ç±»
        
        # è·å–å½“å‰æ—¶é—´å¯¹è±¡
        now = get_current_jst_time()
        
        # å°è¯•è§£æ RSS è‡ªå¸¦çš„æ—¶é—´
        try:
            if hasattr(entry, 'published_parsed'):
                # entry.published_parsed æ˜¯ UTC æ—¶é—´ï¼Œéœ€è½¬ä¸º JST
                pub_tm = entry.published_parsed
                dt_utc = datetime.datetime(*pub_tm[:6])
                dt_jst = dt_utc + datetime.timedelta(hours=9)
            else:
                dt_jst = now
        except:
            dt_jst = now

        # æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²
        time_display = dt_jst.strftime("%m-%d %H:%M") # æ˜¾ç¤ºä¸º 11-28 10:00
        timestamp = dt_jst.timestamp() # ç”¨äºæ’åºçš„æ•°å­—

        item = {
            "title": zh_title,
            "origin": original_title,
            "link": entry.link,
            "time_str": time_display,
            "timestamp": timestamp, # æ’åºç”¨
            "image": image_url,
            "category": category
        }
        processed.append(item)
        time.sleep(0.2)
        
    return processed

def update_news():
    # 1. æŠ“å–ä»Šæ—¥æœ€æ–°
    raw_entries = fetch_google_china_news()
    new_data = process_entries(raw_entries)

    # 2. å­˜å…¥ä»Šæ—¥å­˜æ¡£
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
    
    today = get_current_jst_time()
    today_str = today.strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{today_str}.json")
    
    # è¯»å–æ—§çš„ä»Šæ—¥å­˜æ¡£ï¼ˆåˆå¹¶å»é‡ï¼‰
    final_today_list = []
    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                final_today_list = json.load(f)
        except:
            pass

    # åˆå¹¶é€»è¾‘
    existing_links = set(i['link'] for i in final_today_list)
    for item in new_data:
        if item['link'] not in existing_links:
            final_today_list.insert(0, item) # æ–°çš„æ”¾å‰é¢
    
    # ä¿å­˜ä»Šæ—¥å­˜æ¡£
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(final_today_list, f, ensure_ascii=False, indent=2)
    print(f"âœ… ä»Šæ—¥å­˜æ¡£æ›´æ–° ({len(final_today_list)}æ¡)")

    # 3. ç”Ÿæˆé¦–é¡µæ•°æ® (èšåˆè¿‡å» 30 å¤©)
    print("ğŸ”„ æ­£åœ¨èšåˆè¿‘ 30 å¤©æ•°æ®...")
    home_data = []
    seen_links = set()

    # å€’åºéå†è¿‡å» 30 å¤© (ä»Šå¤© -> 30å¤©å‰)
    for i in range(30):
        target_date = today - datetime.timedelta(days=i)
        d_str = target_date.strftime("%Y-%m-%d")
        f_path = os.path.join(archive_dir, f"{d_str}.json")
        
        if os.path.exists(f_path):
            try:
                with open(f_path, 'r', encoding='utf-8') as f:
                    day_data = json.load(f)
                    for item in day_data:
                        if item['link'] not in seen_links:
                            home_data.append(item)
                            seen_links.add(item['link'])
            except:
                pass
    
    # é»˜è®¤æŒ‰çƒ­åº¦/RSSé¡ºåºä¿ç•™ (æˆ–è€…æŒ‰æ—¶é—´æ’ï¼Œè¿™é‡Œå…ˆä¿æŒRSSåŸåºï¼Œå‰ç«¯è´Ÿè´£æ’åº)
    # Google RSS æœ¬èº«å°±æ˜¯æŒ‰â€œç›¸å…³æ€§/çƒ­åº¦â€æ’åºçš„
    
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(home_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… data.json æ›´æ–°å®Œæ¯• (åŒ…å« {len(home_data)} æ¡æ–°é—»)")

if __name__ == "__main__":
    update_news()