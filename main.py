import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time
import requests
from bs4 import BeautifulSoup

# è®¾ç½®æ—¶åŒº UTC+9
JST_OFFSET = datetime.timedelta(hours=9)

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

# --- å›¾ç‰‡æå–é€»è¾‘ ---
def extract_image(entry):
    # 1. Bing çš„ media_content / media_thumbnail
    if 'media_content' in entry:
        for media in entry.media_content:
            if 'image' in media.get('type', '') or 'medium' in media:
                return media.get('url', '')
    if 'media_thumbnail' in entry and len(entry.media_thumbnail) > 0:
        return entry.media_thumbnail[0].get('url', '')

    # 2. Bing/Google çš„ links
    if 'links' in entry:
        for link in entry.links:
            if link.get('type', '').startswith('image/'):
                return link.get('href', '')

    # 3. HTML å†…å®¹æå– (Google News å¿…å¤‡)
    content_html = ""
    if 'summary' in entry:
        content_html = entry.summary
    elif 'description' in entry:
        content_html = entry.description
    
    if content_html:
        try:
            soup = BeautifulSoup(content_html, 'html.parser')
            img = soup.find('img')
            if img and 'src' in img.attrs:
                return img['src']
        except:
            pass
    return ""

def fetch_feed(source_type, url):
    print(f"ğŸš€ æ­£åœ¨æŠ“å– [{source_type}] ...")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            print(f"âŒ [{source_type}] è¯·æ±‚è¢«æ‹’ç»: {response.status_code}")
            return []
        
        feed = feedparser.parse(response.content)
        if not feed.entries:
            print(f"âš ï¸ [{source_type}] RSS è§£ææˆåŠŸä½†æ— å†…å®¹")
            return []
            
        print(f"âœ… [{source_type}] è·å–åˆ° {len(feed.entries)} æ¡åŸå§‹æ•°æ®")
        return feed.entries
    except Exception as e:
        print(f"âŒ [{source_type}] ç½‘ç»œ/è§£æé”™è¯¯: {e}")
        return []

def process_entries(entries, category_label):
    processed = []
    translator = GoogleTranslator(source='auto', target='zh-CN')
    
    # é™åˆ¶æŠ“å–æ•°é‡
    limit = 20 if category_label == "china" else 15
    
    for entry in entries[:limit]:
        original_title = entry.title
        
        # --- æ¥æºæå– ---
        # Google News çš„æ ‡é¢˜é€šå¸¸æ˜¯ "æ ‡é¢˜ - åª’ä½“å"
        # æˆ‘ä»¬æŠŠåª’ä½“åæå–å‡ºæ¥ï¼Œä¸ºäº†è¯æ˜è¿™æ˜¯æ—¥æœ¬åª’ä½“
        media_name = ""
        clean_title = original_title
        if ' - ' in original_title:
            parts = original_title.rsplit(' - ', 1)
            clean_title = parts[0]
            media_name = parts[1]
        elif 'source' in entry:
            media_name = entry.source.title

        # 1. ç¿»è¯‘æ ‡é¢˜
        try:
            zh_title = translator.translate(clean_title)
        except:
            zh_title = clean_title 

        # 2. æå–å›¾ç‰‡
        image_url = extract_image(entry)
        
        # Bingå›¾ç‰‡ä¿®å¤ (å»é™¤ç¼©æ”¾å‚æ•°æ‹¿åŸå›¾)
        if 'bing.net' in image_url or 'th?id=' in image_url:
            if '&w=' in image_url:
                image_url = image_url.split('&w=')[0]

        time_str = get_current_jst_time().strftime("%H:%M")
        
        # æ„é€ æ˜¾ç¤ºç”¨çš„æ¥æºå­—ç¬¦ä¸²
        origin_display = original_title
        if media_name:
            # è¿™é‡Œçš„ç›®çš„æ˜¯è®©ç”¨æˆ·åœ¨ç•Œé¢ä¸Šçœ‹åˆ° [NHK] è¿™æ ·çš„å­—æ ·
            # æˆ‘ä»¬ä¸ä¿®æ”¹ origin å­—æ®µçš„å­˜å‚¨ï¼Œä½†åœ¨å‰ç«¯å¯èƒ½éœ€è¦ç•™æ„ï¼Œæˆ–è€…ç›´æ¥å­˜å…¥ origin
            # è¿™é‡Œç®€å•å¤„ç†ï¼Œç›´æ¥æŠŠåŸæ–‡æ ‡é¢˜è®¾ä¸ºåŒ…å«æ¥æºçš„
            pass 

        item = {
            "title": zh_title,
            "origin": original_title, # ä¿ç•™åŒ…å«åª’ä½“åçš„å®Œæ•´æ ‡é¢˜
            "link": entry.link,
            "time": time_str,
            "image": image_url
        }
        processed.append(item)
        time.sleep(0.2)
        
    return processed

def update_news():
    # --- 1. å®šä¹‰æº ---
    # æ—¥æœ¬çƒ­æœ (Bing): ç¨³å®šï¼Œå¸¦å›¾
    BING_HOT_URL = "https://www.bing.com/news/search?q=&format=rss&cc=JP"
    
    # ä¸­å›½ç›¸å…³ (Google News æ—¥æœ¬ç‰ˆ): 
    # hl=ja (æ—¥è¯­)
    # gl=JP (æ—¥æœ¬åœ°åŒº)
    # ceid=JP:ja (å¼ºåˆ¶ä½¿ç”¨æ—¥æœ¬ç‰ˆå¼•æ“ -> å…³é”®ï¼è¿™ä¿è¯äº†æ¥æºéƒ½æ˜¯æ—¥æœ¬åª’ä½“)
    # å»æ‰äº† when:1d ä»¥ä¿è¯æœ‰æ•°æ®
    GOOGLE_CHINA_URL = "https://news.google.com/rss/search?q=ä¸­å›½&hl=ja&gl=JP&ceid=JP:ja"

    # --- 2. æŠ“å– ---
    raw_hot = fetch_feed("æ—¥æœ¬çƒ­æœ(Bing)", BING_HOT_URL)
    raw_china = fetch_feed("ä¸­å›½ç›¸å…³(Google)", GOOGLE_CHINA_URL)

    # --- 3. å¤„ç† ---
    hot_data = process_entries(raw_hot, "hot")
    china_data = process_entries(raw_china, "china")

    # --- 4. è¯»å†™å­˜æ¡£ ---
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
    
    date_str = get_current_jst_time().strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{date_str}.json")
    
    final_data = { "hot": [], "china": [] }

    # è¯»å–å¹¶åˆå¹¶
    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                old = json.load(f)
                if isinstance(old, dict):
                    final_data = old
        except:
            pass

    def merge(old_list, new_list):
        seen = set(i['link'] for i in old_list)
        for item in new_list:
            if item['link'] not in seen:
                old_list.insert(0, item)
        return old_list[:40]

    final_data['hot'] = merge(final_data.get('hot', []), hot_data)
    final_data['china'] = merge(final_data.get('china', []), china_data)

    print(f"âœ… æœ€ç»ˆå…¥åº“: çƒ­æœ {len(final_data['hot'])} æ¡, ä¸­å›½ {len(final_data['china'])} æ¡")

    # å†™å…¥
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    print("âœ… data.json æ›´æ–°å®Œæ¯•")

if __name__ == "__main__":
    update_news()